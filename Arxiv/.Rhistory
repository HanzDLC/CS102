titles <- c(titles, currentTitles)
users <- c(users, currentUsers)
dates <- c(dates, currentDates)
contents <- c(contents, currentContents)
stars <- c(stars, currentStars)
}
reviews_data <- data.frame(
reviewTitle = titles,
userName = users,
reviewDate = dates,
reviewContent = contents,
numStars = stars
)
return(reviews_data)
}
movieURL <- 'https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ql_2'
allReviews <- scrapeReviews(movieURL)
scrapeReviews <- function(url, numReviews = 300) {
titles <- character(0)
users <- character(0)
dates <- character(0)
contents <- character(0)
stars <- character(0)
numPages <- ceiling(numReviews / 25)
for (page in 1:numPages) {
currentPageURL <- modify_url(url, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
pageResponse <- httr::GET(currentPageURL)
currentTitles <- content(pageResponse, "text") %>%
read_html() %>%
html_node('a.title') %>%
html_text()
currentUsers <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.review-date') %>%
html_text()
currentContents <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.point-scale') %>%
html_attr("aria-label")
currentStars <- ifelse(length(currentStars) == 0, "N/A", currentStars)
titles <- c(titles, currentTitles)
users <- c(users, currentUsers)
dates <- c(dates, currentDates)
contents <- c(contents, currentContents)
stars <- c(stars, currentStars)
}
reviews_data <- data.frame(
reviewTitle = titles,
userName = users,
reviewDate = dates,
reviewContent = contents,
numStars = stars
)
return(reviews_data)
}
movieURL <- 'https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ql_2'
allReviews <- scrapeReviews(movieURL)
scrapeReviews <- function(url, numReviews = 300) {
titles <- character(0)
users <- character(0)
dates <- character(0)
contents <- character(0)
stars <- character(0)
numPages <- ceiling(numReviews / 25)
for (page in 1:numPages) {
currentPageURL <- modify_url(url, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
pageResponse <- httr::GET(currentPageURL)
currentTitles <- content(pageResponse, "text") %>%
read_html() %>%
html_node('a.title') %>%
html_text()
currentUsers <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.review-date') %>%
html_text()
currentContents <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- content(pageResponse, "text") %>%
read_html() %>%
html_node('.point-scale') %>%
html_attr("aria-label")
titles <- c(titles, currentTitles)
users <- c(users, currentUsers)
dates <- c(dates, currentDates)
contents <- c(contents, currentContents)
stars <- c(stars, currentStars)
}
reviews_data <- data.frame(
reviewTitle = titles,
userName = users,
reviewDate = dates,
reviewContent = contents,
numStars = stars
)
return(reviews_data)
}
movieURL <- 'https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ql_2'
allReviews <- scrapeReviews(movieURL)
head(allReviews)
View(allReviews)
library(ggplot2)
library(dplyr)
library(polite)
library(xml2)
library(magrittr)
library(rvest)
polite::use_manners(save_as ="polite_scrape.R")
movieURL <-  'https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ql_2'
session <- polite::bow(movieURL, user_agent = 'Educational')
reviewTitle <- character(0)
userName <- character(0)
reviewDate <- character(0)
metaScore <- character(0)
reviewContent <- character(0)
numStars <- character(0)
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
urrentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
titles <- c(titles, currentTitles)
reviewtitles <- c(titles, currentTitles)
reviewtitles <- c(reviewtitles, currentTitles)
reviewTitles <- c(reviewtiTles, currentTitles)
reviewTitles <- c(reviewTitles, currentTitles)
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
library(ggplot2)
library(dplyr)
library(polite)
library(xml2)
library(magrittr)
library(rvest)
polite::use_manners(save_as ="polite_scrape.R")
movieURL <-  'https://www.imdb.com/title/tt0111161/reviews/?ref_=tt_ql_2'
session <- polite::bow(movieURL, user_agent = 'Educational')
reviewTitle <- character(0)
userName <- character(0)
reviewDate <- character(0)
metaScore <- character(0)
reviewContent <- character(0)
numStars <- character(0)
for (page in 1:numPages) {
currentPageURL <- modify_url(url, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
numReviews = 300
for (page in 1:numPages) {
currentPageURL <- modify_url(url, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
numReviews = 300
numPages <- ceiling(numReviews / 25)
for (page in 1:numPages) {
currentPageURL <- modify_url(url, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
numReviews = 300
numPages <- ceiling(numReviews / 25)
for (page in 1:numPages) {
currentPageURL <- modify_url(movieURL, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
numStars
reviewContent
userName
numReviews = 300
numPages <- ceiling(numReviews / 25)
reviewTitle <- c()
userName <- c()
reviewDate <- c()
reviewContent <- c()
numStars <- c()
for (page in 1:numPages) {
currentPageURL <- modify_url(movieURL, query = list(start = (page - 1) * 25 + 1))
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
userName
numReviews = 300
numPages <- ceiling(numReviews / 25)
reviewTitle <- chracter(0)
numReviews = 300
numPages <- ceiling(numReviews / 25)
reviewTitle <- character(0)
userName <- character(0)
reviewDate <- character(0)
reviewContent <- character(0)
# Function to extract pagination key from URL
getPaginationKey <- function(url) {
parsed_url <- url_parse(url)
query_params <- url_query(parsed_url)
return(query_params$start)
}
for (page in 1:numPages) {
currentPageURL <- modify_url(movieURL, query = list(start = (page - 1) * 25 + 1))
paginationKey <- getPaginationKey(currentPageURL)
Sys.sleep(2) #prevent the server from overloading
parentNode <- scrape(session) %>%
html_elements('div.review-container')
currentTitles <- parentNode %>%
html_node('a.title') %>%
html_text()
currentUsers <- parentNode %>%
html_node('.display-name-link') %>%
html_text()
currentDates <- parentNode %>%
html_node('.review-date') %>%
html_text()
currentContents <- parentNode %>%
html_node('.text.show-more__control') %>%
html_text()
currentStars <- parentNode %>%
html_node('.ipl-ratings-bar') %>%
html_text()
reviewTitle <- c(reviewTitle, currentTitles)
userName <- c(userName, currentUsers)
reviewDate <- c(reviewDate, currentDates)
reviewContent <- c(reviewContent, currentContents)
numStars <- c(numStars, currentStars)
}
library(dplyr)
library(stringr)
library(httr)
library(rvest)
url <- 'https://arxiv.org/search/?query=%22text+mining%22&searchtype=all&source=header&start=0'
parse_url(url)
start <- proc.time()
title <- NULL
author <- NULL
subject <- NULL
abstract <- NULL
meta <- NULL
library(dplyr)
library(stringr)
library(httr)
library(rvest)
url <- 'https://arxiv.org/search/cs?query=systems&searchtype=all&abstracts=show&order=-announced_date_first&size=50'
parse_url(url)
start <- proc.time()
title <- NULL
author <- NULL
subject <- NULL
abstract <- NULL
meta <- NULL
pages <- seq(from = 0, to = 10, by = 10)
for( i in pages){
tmp_url <- modify_url(url, query = list(start = i))
tmp_list <- read_html(tmp_url) %>%
html_nodes('p.list-title.is-inline-block') %>%
html_nodes('a[href^="https://arxiv.org/abs"]') %>%
html_attr('href')
for(j in 1:10){
tmp_paragraph <- read_html(tmp_list[j])
# title
tmp_title <- tmp_paragraph %>% html_nodes('h1.title.mathjax') %>% html_text(T)
tmp_title <-  gsub('Title:', '', tmp_title)
title <- c(title, tmp_title)
# author
tmp_author <- tmp_paragraph %>% html_nodes('div.authors') %>% html_text
tmp_author <- gsub('\\s+',' ',tmp_author)
tmp_author <- gsub('Authors:','',tmp_author) %>% str_trim
author <- c(author, tmp_author)
# subject
tmp_subject <- tmp_paragraph %>% html_nodes('span.primary-subject') %>% html_text(T)
subject <- c(subject, tmp_subject)
# abstract
tmp_abstract <- tmp_paragraph %>% html_nodes('blockquote.abstract.mathjax') %>% html_text(T)
tmp_abstract <- gsub('\\s+',' ',tmp_abstract)
tmp_abstract <- sub('Abstract:','',tmp_abstract) %>% str_trim
abstract <- c(abstract, tmp_abstract)
# meta
tmp_meta <- tmp_paragraph %>% html_nodes('div.submission-history') %>% html_text
tmp_meta <- lapply(strsplit(gsub('\\s+', ' ',tmp_meta), '[v1]', fixed = T),'[',2) %>% unlist %>% str_trim
meta <- c(meta, tmp_meta)
cat(j, "paper\n")
Sys.sleep(1)
}
cat((i/10) + 1,'/ 1 page\n')
}
papers <- data.frame(title, author, subject, abstract, meta)
# Export the result
save(papers, file = "Arxiv_Text_Mining.RData")
write.csv(papers, file = "ArxivPapersOnTextMining.csv")
)
setwd ("C:\Users\63927\Downloads\Arxiv")
setwd ("C:/Users/63927/Downloads/Arxiv")
# Export the result
save(papers, file = "Arxiv_Text_Mining.RData")
write.csv(papers, file = "ArxivPapersOnTextMining.csv")
knitr::opts_chunk$set(echo = TRUE)
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "sample_db",
user = "root",
password = PWD) #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = ) #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = "") #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = " ") #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = "") #PWD is the password of your MySQL
library(dplyr,dbplyr)
library(RMySQL)
library(RMariaDB)
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = "") #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = " ") #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::MySQL(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = " ") #PWD is the password of your MySQL
#creating connections
connection <- dbConnect(RMariaDB::,
#creating connections
connection <- dbConnect(RMariaDB::MariaDB(),
dsn="MySQL-connection",
Server = "localhost",
dbname = "arxivdb",
user = "root",
password = "") #PWD is the password of your MySQL
#listing the tables of the sample_db
dbListTables(connection)
#listing the fields of the authors in the table sample_db
dbListFields(connection,"arxivdata")
#listing the fields of the authors in the table sample_db
dbListFields(connection,"arxivdata")
dbListFields(connection,"arxivdata")
